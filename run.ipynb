{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“.venv (Python 3.12.3)”的单元格需要ipykernel包。\n",
      "\u001b[1;31m将“ipykernel”安装到 Python 环境中。\n",
      "\u001b[1;31m命令:“/home/yyc/data/.venv/bin/python -m pip install ipykernel -U --force-reinstall”"
     ]
    }
   ],
   "source": [
    "import sys; print(sys.path[:8])\n",
    "\n",
    "import sys, torch\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"torch from:\", torch.__file__)\n",
    "print(\"torch:\", torch.__version__, \"| compiled cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/yyc/data\n",
      "Current working directory: /home/yyc/data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent working directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ROOT)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#from torchdiffeq import odeint\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/__init__.py:46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     ArrowDtype,\n\u001b[1;32m     49\u001b[0m     Int8Dtype,\n\u001b[1;32m     50\u001b[0m     Int16Dtype,\n\u001b[1;32m     51\u001b[0m     Int32Dtype,\n\u001b[1;32m     52\u001b[0m     Int64Dtype,\n\u001b[1;32m     53\u001b[0m     UInt8Dtype,\n\u001b[1;32m     54\u001b[0m     UInt16Dtype,\n\u001b[1;32m     55\u001b[0m     UInt32Dtype,\n\u001b[1;32m     56\u001b[0m     UInt64Dtype,\n\u001b[1;32m     57\u001b[0m     Float32Dtype,\n\u001b[1;32m     58\u001b[0m     Float64Dtype,\n\u001b[1;32m     59\u001b[0m     CategoricalDtype,\n\u001b[1;32m     60\u001b[0m     PeriodDtype,\n\u001b[1;32m     61\u001b[0m     IntervalDtype,\n\u001b[1;32m     62\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     63\u001b[0m     StringDtype,\n\u001b[1;32m     64\u001b[0m     BooleanDtype,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     NA,\n\u001b[1;32m     67\u001b[0m     isna,\n\u001b[1;32m     68\u001b[0m     isnull,\n\u001b[1;32m     69\u001b[0m     notna,\n\u001b[1;32m     70\u001b[0m     notnull,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     Index,\n\u001b[1;32m     73\u001b[0m     CategoricalIndex,\n\u001b[1;32m     74\u001b[0m     RangeIndex,\n\u001b[1;32m     75\u001b[0m     MultiIndex,\n\u001b[1;32m     76\u001b[0m     IntervalIndex,\n\u001b[1;32m     77\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     78\u001b[0m     DatetimeIndex,\n\u001b[1;32m     79\u001b[0m     PeriodIndex,\n\u001b[1;32m     80\u001b[0m     IndexSlice,\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     NaT,\n\u001b[1;32m     83\u001b[0m     Period,\n\u001b[1;32m     84\u001b[0m     period_range,\n\u001b[1;32m     85\u001b[0m     Timedelta,\n\u001b[1;32m     86\u001b[0m     timedelta_range,\n\u001b[1;32m     87\u001b[0m     Timestamp,\n\u001b[1;32m     88\u001b[0m     date_range,\n\u001b[1;32m     89\u001b[0m     bdate_range,\n\u001b[1;32m     90\u001b[0m     Interval,\n\u001b[1;32m     91\u001b[0m     interval_range,\n\u001b[1;32m     92\u001b[0m     DateOffset,\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     to_numeric,\n\u001b[1;32m     95\u001b[0m     to_datetime,\n\u001b[1;32m     96\u001b[0m     to_timedelta,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     Flags,\n\u001b[1;32m     99\u001b[0m     Grouper,\n\u001b[1;32m    100\u001b[0m     factorize,\n\u001b[1;32m    101\u001b[0m     unique,\n\u001b[1;32m    102\u001b[0m     value_counts,\n\u001b[1;32m    103\u001b[0m     NamedAgg,\n\u001b[1;32m    104\u001b[0m     array,\n\u001b[1;32m    105\u001b[0m     Categorical,\n\u001b[1;32m    106\u001b[0m     set_eng_float_format,\n\u001b[1;32m    107\u001b[0m     Series,\n\u001b[1;32m    108\u001b[0m     DataFrame,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "import importlib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 脚本运行时的项目根目录：脚本文件所在目录（在 notebook 里用 Path.cwd()）\n",
    "ROOT = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "\n",
    "DATA_DIR = ROOT  # 你原来就把项目放在 /home/yyc/data\n",
    "print(\"Current working directory:\", ROOT)\n",
    "\n",
    "#from torchdiffeq import odeint\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from model.init import model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model.Multiagent import MultiAgentFusion\n",
    "# GNNs (Graph Neural Network) \n",
    "from model.mlp import MLPModel\n",
    "# MAMBA \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "#from model.mamba import Mamba\n",
    "# LSTM \n",
    "#from model.lstm import LSTM\n",
    "# GRU \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from model.gru import GRUModel\n",
    "# Neural Network \n",
    "#from model.neural import NeuralODE\n",
    "# Transformer \n",
    "#from model.transformer import Transformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "print(torch.__version__)  # 查看torch当前版本号\n",
    "print(torch.version.cuda)  # 编译当前版本的torch使用的cuda版本号\n",
    "print(torch.cuda.is_available())  # 查看当前cuda是否可用于当前版本的Torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个 Dataset 子类，负责加载数据\n",
    "class LargeDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, num_encoder_layers=6, nhead=4)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 输入特征进行嵌入\n",
    "        x = x.unsqueeze(0)  # 加一个 batch 维度\n",
    "        x = self.transformer(x, x)  # Transformer 进行处理\n",
    "        x = x.squeeze(0)  # 移除 batch 维度\n",
    "        x = self.fc(x)  # 输出预测\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./TrafficLabelling/end.csv').values\n",
    "le = LabelEncoder()\n",
    "le.fit(data[:,-1])\n",
    "\n",
    "data[:,-1]= le.transform(data[:,-1])\n",
    "num_classes = len(np.unique(data[:,-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        #print(f\"Input shape before unsqueeze: {input_seq.shape}\")  # Debug: Check input shape\n",
    "        if len(input_seq.shape) == 2:\n",
    "            input_seq = input_seq.unsqueeze(1)\n",
    "            #print(f\"Input shape after unsqueeze: {input_seq.shape}\")  # Debug: Should now be 3D\n",
    "\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        #print(f\"LSTM output shape: {lstm_out.shape}\")  # Debug: [batch_size, seq_len, hidden_layer_size]\n",
    "        lstm_out_last = lstm_out[:, -1, :]\n",
    "        predictions = self.linear(lstm_out_last)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, n_classes, d_model=16, d_state=2, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand, use_fast_path=False)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.input_embedding(x)\n",
    "        #print(f\"After input_embedding: {x}\")  # Debugging step\n",
    "        x = self.mamba(x)\n",
    "        #print(f\"After Mamba: {x}\")  # Debugging step\n",
    "        x = x[:, -1, :]  # Take last time step\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-1].astype('float64'), data[:,-1].astype('float64'), test_size=0.2)\n",
    "class_0_indices = np.where(y_train == 0)[0]\n",
    "other_classes_indices = np.where(y_train != 0)[0]\n",
    "\n",
    "# Randomly select half of the '0' class samples\n",
    "reduced_class_0_indices = np.random.choice(class_0_indices, size=len(class_0_indices) // 100, replace=False)\n",
    "\n",
    "# Combine reduced '0' class indices with all other class indices\n",
    "balanced_indices = np.concatenate([reduced_class_0_indices, other_classes_indices])\n",
    "X_train[np.isinf(X_train)] = 0\n",
    "X_train[np.isnan(X_train)] = 0\n",
    "X_test[np.isinf(X_test)] = 0\n",
    "X_test[np.isnan(X_test)] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the new balanced dataset\n",
    "X_train= X_train[balanced_indices]\n",
    "y_train= y_train[balanced_indices]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train=X_train\n",
    "X_test=X_test\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "dimension = X_train.shape[1]\n",
    "print(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "num_classes = len(np.unique(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "plt.hist(y_train, bins=np.arange(num_classes + 1) - 0.5, edgecolor='black')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 定义一个简单的全连接网络作为ODE的右侧函数\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        神经网络模型的ODE右侧函数\n",
    "        参数：\n",
    "        - t: 时间点（在ODE求解中自动传递）\n",
    "        - x: 当前输入\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x  # 返回logits\n",
    "\n",
    "\n",
    "    def solve(self, x0, t):\n",
    "        \"\"\"\n",
    "        使用odeint求解ODE，得到模型的输出\n",
    "        参数：\n",
    "        - x0: 初始状态\n",
    "        - t: 时间序列\n",
    "        \"\"\"\n",
    "        # odeint通过自动微分求解ODE\n",
    "        solution = odeint(self, x0, t)\n",
    "        return solution[-1]  # 取最后时刻的输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, optimizer, loss_fn, device):\n",
    "        \"\"\"\n",
    "        初始化Agent。\n",
    "\n",
    "        参数：\n",
    "        - model: 该Agent的模型\n",
    "        - optimizer: 优化器\n",
    "        - loss_fn: 损失函数\n",
    "        - device: 设备（'cpu' 或 'cuda'）\n",
    "        \"\"\"\n",
    "        self.model = model # 将模型移动到指定设备\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.reward = 0.0  # 初始化奖励值\n",
    "        self.device = device\n",
    "        self.weight = 1.0  # 初始化权重\n",
    "     \n",
    "    def train_NeuralODE(self, inputs, target):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        t = torch.linspace(0, 1, steps=100).to(self.device)\n",
    "        # 假设 forward 方法期望的是 'inputs' 而非 'x'\n",
    "        outputs = self.model(t,inputs)  # 使用正确的参数名传入\n",
    "        if target.dtype != torch.long:\n",
    "            target = target.long() \n",
    "        loss = self.loss_fn(outputs, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    def train(self, inputs, target):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # 假设 forward 方法期望的是 'inputs' 而非 'x'\n",
    "        outputs = self.model(inputs)  # 使用正确的参数名传入\n",
    "        if target.dtype != torch.long:\n",
    "            target = target.long() \n",
    "        loss = self.loss_fn(outputs, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    def predict_NeuralODE(self, X):\n",
    "        # 获取当前设备\n",
    "        device = next(self.model.parameters()).device  # 获取模型参数所在设备\n",
    "\n",
    "        # 创建时间序列 t，并移动到相同设备\n",
    "        t = torch.linspace(0, 1, steps=10, device=device)  # 假设时间点 t 需要在相同设备上\n",
    "\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=device)  # 将 X 移动到同一个设备\n",
    "\n",
    "        # 使用相同设备进行前向传播\n",
    "        outputs = self.model(t, X)  # 模型前向传播\n",
    "        \n",
    "        return outputs.detach().cpu().numpy()  # 返回 CPU 上的 numpy 数组\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 获取模型所在的设备\n",
    "        device = next(self.model.parameters()).device  # 获取模型参数所在的设备\n",
    "\n",
    "        if isinstance(X, torch.Tensor):  # 如果 X 已经是 tensor 类型\n",
    "            X = X.to(device)  # 将 X 移动到模型所在的设备\n",
    "        else:\n",
    "            # 如果 X 是 numpy 数组，先转为 tensor，并将其移动到正确设备\n",
    "            X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "\n",
    "        # 使用与设备一致的张量进行预测\n",
    "        outputs = self.model(X)  # 模型前向传播\n",
    "        return outputs.detach().cpu().numpy()  # 返回 CPU 上的 numpy 数组\n",
    "\n",
    "    def update_reward(self, reward):\n",
    "        \"\"\"\n",
    "        更新Agent的奖励值。\n",
    "\n",
    "        参数：\n",
    "        - reward: 新的奖励值\n",
    "        \"\"\"\n",
    "        self.reward += reward\n",
    "\n",
    "    def adjust_learning_rate(self, min_lr=0.001, max_lr=0.01):\n",
    "        \"\"\"\n",
    "        根据奖励动态调整学习率。\n",
    "\n",
    "        参数：\n",
    "        - min_lr: 最小学习率\n",
    "        - max_lr: 最大学习率\n",
    "        \"\"\"\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            new_lr = max(min_lr, min(max_lr, param_group['lr'] + self.reward * 0.001))\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    def adjust_weight(self):\n",
    "        \"\"\"\n",
    "        根据奖励值调整Agent的权重。\n",
    "        \"\"\"\n",
    "        self.weight = max(0.1, self.reward)  # 确保最小权重为0.1\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_weights(agents):\n",
    "        \"\"\"\n",
    "        归一化所有Agent的权重，使得权重之和为1。\n",
    "\n",
    "        参数：\n",
    "        - agents: 所有Agent的列表\n",
    "        \"\"\"\n",
    "        total_reward = sum(agent.reward for agent in agents)\n",
    "        if total_reward > 0:\n",
    "            for agent in agents:\n",
    "                agent.weight = agent.reward / total_reward\n",
    "        else:\n",
    "            for agent in agents:\n",
    "                agent.weight = 1.0 / len(agents)  # 如果没有奖励，均分权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = MultiAgentFusion(input_size=5, hidden_size=64, output_size=num_classes)\n",
    "mlp_model = MLPModel(input_size=dimension, hidden_size=64, output_size=num_classes)\n",
    "gru_model = GRUModel(input_size=dimension, hidden_layer_size=64, num_layers=2, output_size=num_classes)\n",
    "lstm_model = LSTM(input_size=dimension, hidden_layer_size=64, num_layers=2, output_size=num_classes)\n",
    "batch, length, dim = 2, 64, 768\n",
    "#mamba_model = MambaPredictor(input_dim=dimension, seq_len=1, n_classes=num_classes)\n",
    "neural_model = NeuralODE(input_dim=dimension, hidden_dim=1, output_dim=num_classes)\n",
    "transformer_model = Transformer(input_size=dimension, hidden_size=4, num_classes=num_classes)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义保存模型的函数\n",
    "def save_model(agent, agent_id, epoch, save_dir=\"saved_models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # 创建保存目录\n",
    "    model_filename = f\"agent_{agent_id}_epoch_{epoch}.pth\"\n",
    "    model_path = os.path.join(save_dir, model_filename)\n",
    "    \n",
    "    # 保存模型的state_dict\n",
    "    torch.save(agent.model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "# 创建每个代理时传入设备\n",
    "agents = [\n",
    "    \n",
    "    Agent(\n",
    "        model=neural_model,\n",
    "        optimizer=optim.Adam(neural_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device=device\n",
    "    ),\n",
    "    Agent(\n",
    "        model=lstm_model,\n",
    "        optimizer=optim.Adam(lstm_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device=device\n",
    "    ),\n",
    "    Agent(\n",
    "        model=mlp_model,\n",
    "        optimizer=optim.Adam(mlp_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device=device\n",
    "    ),\n",
    "    Agent(\n",
    "        model=gru_model,\n",
    "        optimizer=optim.Adam(gru_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device=device\n",
    "    ),\n",
    "  \n",
    "    Agent(\n",
    "        model=transformer_model,\n",
    "        optimizer=optim.Adam(transformer_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device=device\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Main loop for training, fusion, feedback, and optimization\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "accuracy_scores = []\n",
    "agent_outputs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_predict_wrapper(agent, X):\n",
    "    # 确保输入是正确的\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    return agent.model(X).detach().cpu().numpy()  # 返回的是一个 numpy 数组\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    # 记录loss\n",
    "    agent_outputs = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        losses = []\n",
    "        for i, agent in enumerate(agents):\n",
    "            if i==0:\n",
    "                print(f\"Training Agent {i + 1}\")\n",
    "                agent_loss = []\n",
    "                #print(data)\n",
    "                #print(target)\n",
    "                loss = agent.train_NeuralODE(data, target)  # 改为单批次训练\n",
    "                agent_loss.append(loss)  # 将每个batch的loss存入列表\n",
    "                if batch_idx % 100 == 0:  # 每100个batch打印一次\n",
    "                    print(f\"Batch {batch_idx}, Loss: {loss}\")\n",
    "                # 将当前agent的所有loss加入到全局loss记录中\n",
    "                losses.append(np.mean(agent_loss))  # 记录当前agent每个epoch的loss平均值\n",
    "                agent_outputs.append(agent.predict_NeuralODE(X_test))\n",
    "                # 每个epoch结束后保存模型\n",
    "                #save_model(agent, i, 1)    \n",
    "            else:\n",
    "                print(f\"Training Agent {i + 1}\")\n",
    "                agent_loss = []\n",
    "                #print(f\"Batch {batch_idx}, Data: {data}\")  # 打印数据结构\n",
    "                loss = agent.train(data, target)  # 改为单批次训练\n",
    "                agent_loss.append(loss)  # 将每个batch的loss存入列表\n",
    "                if batch_idx % 100 == 0:  # 每100个batch打印一次\n",
    "                    print(f\"Batch {batch_idx}, Loss: {loss}\")\n",
    "                # 将当前agent的所有loss加入到全局loss记录中\n",
    "                losses.append(np.mean(agent_loss))  # 记录当前agent每个epoch的loss平均值\n",
    "                # Generate predictions for FB distribution\n",
    "                agent_outputs.append(agent.predict(X_test))\n",
    "                # 每个epoch结束后保存模型\n",
    "                #save_model(agent, i, 1)     \n",
    "    # 将 numpy 数组改为 torch 张量，并且确保这些张量有梯度\n",
    "    fb_distribution = torch.mean(torch.tensor(agent_outputs, dtype=torch.float32), dim=0, keepdim=False)  # 使用 torch.mean 计算均值\n",
    "    # 进一步训练基于 FB 分布反馈\n",
    "    for i, agent in enumerate(agents):\n",
    "        if i >= len(agent_outputs):  # 防止越界访问\n",
    "            print(f\"Warning: agent_outputs does not have an output for agent {i + 1}\")\n",
    "            continue  # 跳过当前 agent\n",
    "        # 确保 agent_outputs 和 fb_distribution 是 Torch 张量并且需要梯度\n",
    "        raw_output = torch.tensor(agent_outputs[i], dtype=torch.float32, requires_grad=True)  # 使 raw_output 具有梯度\n",
    "        fb_distribution_tensor = fb_distribution  # 此时 fb_distribution 已经是 Torch 张量，继续使用\n",
    "        # 使用损失函数计算损失\n",
    "        fb_loss = agent.loss_fn(raw_output, fb_distribution_tensor)\n",
    "        # 反向传播\n",
    "        agent.optimizer.zero_grad()\n",
    "        fb_loss.backward()  # 现在可以正常计算梯度\n",
    "        agent.optimizer.step()\n",
    "    # Shapley value calculation\n",
    "    shapley_values = []\n",
    "    for i, agent in enumerate(agents):\n",
    "        # Randomly sample from X_train for Shapley calculation\n",
    "        if(i==0):\n",
    "            sample_idx = np.random.choice(len(X_train), size=100, replace=False)\n",
    "            X_sample = X_train[sample_idx]\n",
    "            explainer = shap.KernelExplainer(agent.predict_NeuralODE, X_sample)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shapley_values.append(np.mean(shap_values))\n",
    "        else:\n",
    "            sample_idx = np.random.choice(len(X_train), size=100, replace=False)\n",
    "            X_sample = X_train[sample_idx]\n",
    "            explainer = shap.KernelExplainer(agent.predict, X_sample)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shapley_values.append(np.mean(shap_values))\n",
    "    # Reward and learning rate adjustment\n",
    "    total_shapley = sum(shapley_values)\n",
    "    for i, agent in enumerate(agents):\n",
    "        reward = shapley_values[i] / total_shapley if total_shapley > 0 else 0\n",
    "        agent.update_reward(reward)\n",
    "        agent.adjust_weight()\n",
    "        agent.adjust_learning_rate()\n",
    "\n",
    "    # Display rewards\n",
    "    for i, agent in enumerate(agents):\n",
    "        print(f\"Agent {i + 1} reward after iteration {epoch + 1}: {agent.reward}\")\n",
    "    # epoch结束\n",
    "    print(f\"Epoch {epoch+1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for agent_name, losses in loss_curves.items():\n",
    "    plt.plot(range(1, num_iterations + 1), losses, label=agent_name)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve for Each Agent')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvcc --version  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
