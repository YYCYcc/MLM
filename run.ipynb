{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /root/data\n",
      "['gzh']\n",
      "/root/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n",
      "11.7\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "import importlib\n",
    "\n",
    "import sys\n",
    "sys.path.append('/')\n",
    "print(os.listdir('/data'))\n",
    "print(os.getcwd())\n",
    "from torchdiffeq import odeint\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from model.init import model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model.Multiagent import MultiAgentFusion\n",
    "# GNNs (Graph Neural Network) \n",
    "from model.mlp import MLPModel\n",
    "# MAMBA \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "#from model.mamba import Mamba\n",
    "# LSTM \n",
    "#from model.lstm import LSTM\n",
    "# GRU \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from model.gru import GRUModel\n",
    "# Neural Network \n",
    "#from model.neural import NeuralODE\n",
    "# Transformer \n",
    "#from model.transformer import Transformer\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "import shap\n",
    "from mamba_ssm import Mamba\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "print(torch.__version__)  # 查看torch当前版本号\n",
    "print(torch.version.cuda)  # 编译当前版本的torch使用的cuda版本号\n",
    "print(torch.cuda.is_available())  # 查看当前cuda是否可用于当前版本的Torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个 Dataset 子类，负责加载数据\n",
    "class LargeDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, num_encoder_layers=6, nhead=4)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 输入特征进行嵌入\n",
    "        x = x.unsqueeze(0)  # 加一个 batch 维度\n",
    "        x = self.transformer(x, x)  # Transformer 进行处理\n",
    "        x = x.squeeze(0)  # 移除 batch 维度\n",
    "        x = self.fc(x)  # 输出预测\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./TrafficLabelling/end.csv').values\n",
    "le = LabelEncoder()\n",
    "le.fit(data[:,-1])\n",
    "\n",
    "data[:,-1]= le.transform(data[:,-1])\n",
    "num_classes = len(np.unique(data[:,-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        #print(f\"Input shape before unsqueeze: {input_seq.shape}\")  # Debug: Check input shape\n",
    "        if len(input_seq.shape) == 2:\n",
    "            input_seq = input_seq.unsqueeze(1)\n",
    "            #print(f\"Input shape after unsqueeze: {input_seq.shape}\")  # Debug: Should now be 3D\n",
    "\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        #print(f\"LSTM output shape: {lstm_out.shape}\")  # Debug: [batch_size, seq_len, hidden_layer_size]\n",
    "        lstm_out_last = lstm_out[:, -1, :]\n",
    "        predictions = self.linear(lstm_out_last)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, n_classes, d_model=16, d_state=32, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)  # 添加时间维度，变为 (B, 1, D)\n",
    "        x = self.input_embedding(x)\n",
    "        x = checkpoint(self.mamba, x)  # 使用 checkpoint 包裹 Mamba 模块\n",
    "        x = x[:, -1, :]  # 使用最后一个时间步\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "data = data.astype(np.float64) \n",
    "data[np.isinf(data)] = 0\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "# 平衡数据集：减少类别 0, 2, 4, 10 的样本数\n",
    "for class_label in [0, 2, 4, 10]:\n",
    "    class_indices = np.where(y == class_label)[0]  # 找到当前类的索引\n",
    "    other_classes_indices = np.where(y != class_label)[0]  # 找到其他类的索引\n",
    "    \n",
    "    # 随机选取当前类的一部分样本\n",
    "    if class_label == 0:\n",
    "        reduced_class_indices = np.random.choice(class_indices, size=len(class_indices) // 40, replace=False)\n",
    "    if class_label == 2:\n",
    "        reduced_class_indices = np.random.choice(class_indices, size=len(class_indices) // 10, replace=False)\n",
    "    if class_label == 4:\n",
    "        reduced_class_indices = np.random.choice(class_indices, size=len(class_indices) // 10, replace=False)\n",
    "    if class_label == 10:\n",
    "        reduced_class_indices = np.random.choice(class_indices, size=len(class_indices) // 10, replace=False)\n",
    "    # 合并减少后的当前类样本与其他类样本\n",
    "    balanced_indices = np.concatenate([reduced_class_indices, other_classes_indices])\n",
    "    \n",
    "    # 重新调整数据\n",
    "    X = X[balanced_indices]\n",
    "    y = y[balanced_indices]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "dimension = X_train.shape[1]\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16,shuffle=False)\n",
    "num_classes = len(np.unique(y_train))\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# 平衡数据集：减少类别 0, 2, 4, 10 的样本数\n",
    "\n",
    "plt.hist(y_train, bins=np.arange(num_classes + 1) - 0.5, edgecolor='black')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 定义一个简单的全连接网络作为ODE的右侧函数\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x  # 返回logits\n",
    "\n",
    "\n",
    "    def solve(self, x0, t):\n",
    "        # odeint通过自动微分求解ODE\n",
    "        solution = odeint(self, x0, t)\n",
    "        return solution[-1]  # 取最后时刻的输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, optimizer, loss_fn, device):\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.reward = 0.0  \n",
    "        self.device = device\n",
    "        self.weight = 1.0  \n",
    "     \n",
    "    def train_NeuralODE(self, inputs, target):\n",
    "        inputs = inputs.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        self.model.to(self.device) \n",
    "        t = torch.linspace(0, 1, steps=100).to(self.device)\n",
    "        outputs = self.model(t, inputs)  # Ensure both are on the same device\n",
    "\n",
    "        # Compute loss\n",
    "        if target.dtype != torch.long:\n",
    "            target = target.long()\n",
    "\n",
    "        loss = self.loss_fn(outputs, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self, inputs, target):\n",
    "        inputs = inputs.to(self.device)\n",
    "        self.model.to(self.device) \n",
    "        # 确保数据在正确设备上\n",
    "        inputs, target = inputs.to(self.device), target.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(inputs)\n",
    "        if target.dtype != torch.long:\n",
    "            target = target.long()\n",
    "        loss = self.loss_fn(outputs, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    def predict_NeuralODE(self, X):\n",
    "        # 获取当前设备\n",
    "        device = next(self.model.parameters()).device  # 获取模型参数所在设备\n",
    "\n",
    "        t = torch.linspace(0, 1, steps=10, device=device)  # 假设时间点 t 需要在相同设备上\n",
    "\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=device)  # 将 X 移动到同一个设备\n",
    "\n",
    "        outputs = self.model(t, X)  # 模型前向传播\n",
    "        \n",
    "        return outputs.detach().cpu().numpy()  # 返回 CPU 上的 numpy 数组\n",
    "\n",
    "    def predict(self, X):\n",
    "        device = next(self.model.parameters()).device  # 获取模型参数所在的设备\n",
    "\n",
    "        if isinstance(X, torch.Tensor):  # 如果 X 已经是 tensor 类型\n",
    "            X = X.to(device)  # 将 X 移动到模型所在的设备\n",
    "        else:\n",
    "            X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        outputs = self.model(X)  # 模型前向传播\n",
    "        return outputs.detach().cpu().numpy()  # 返回 CPU 上的 numpy 数组\n",
    "\n",
    "    def update_reward(self, reward):\n",
    "        self.reward += reward\n",
    "\n",
    "    def adjust_learning_rate(self, min_lr=0.001, max_lr=0.01):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            new_lr = max(min_lr, min(max_lr, param_group['lr'] + self.reward * 0.001))\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    def adjust_weight(self):\n",
    "        self.weight = max(0.1, self.reward)  # 确保最小权重为0.1\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_weights(agents):\n",
    "        total_reward = sum(agent.reward for agent in agents)\n",
    "        if total_reward > 0:\n",
    "            for agent in agents:\n",
    "                agent.weight = agent.reward / total_reward\n",
    "        else:\n",
    "            for agent in agents:\n",
    "                agent.weight = 1.0 / len(agents)  # 如果没有奖励，均分权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = MultiAgentFusion(input_size=5, hidden_size=64, output_size=num_classes)\n",
    "mlp_model = MLPModel(input_size=dimension, hidden_size=2, output_size=num_classes)\n",
    "gru_model = GRUModel(input_size=dimension, hidden_layer_size=2, num_layers=2, output_size=num_classes)\n",
    "lstm_model = LSTM(input_size=dimension, hidden_layer_size=2, num_layers=2, output_size=num_classes)\n",
    "batch, length, dim = 2, 64, 768\n",
    "mamba_model = MambaPredictor(input_dim=dimension, seq_len=1, n_classes=num_classes)\n",
    "neural_model = NeuralODE(input_dim=dimension, hidden_dim=1, output_dim=num_classes)\n",
    "#transformer_model = Transformer(input_size=dimension, hidden_size=4, num_classes=num_classes)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义保存模型的函数\n",
    "def save_model(agent, agent_id, epoch, save_dir=\"saved_models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # 创建保存目录\n",
    "    model_filename = f\"agent_{agent_id}_epoch_{epoch}.pth\"\n",
    "    model_path = os.path.join(save_dir, model_filename)\n",
    "    \n",
    "    # 保存模型的state_dict\n",
    "    torch.save(agent.model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "# 创建每个代理时传入设备\n",
    "agents = [\n",
    "    \n",
    "    Agent(\n",
    "        model=neural_model,\n",
    "        optimizer=optim.Adam(neural_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device='cpu'\n",
    "    ),\n",
    "    Agent(\n",
    "        model=lstm_model,\n",
    "        optimizer=optim.Adam(lstm_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device='cpu'\n",
    "    ),\n",
    "    Agent(\n",
    "        model=mlp_model,\n",
    "        optimizer=optim.Adam(mlp_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device='cpu'\n",
    "    ),\n",
    "    Agent(\n",
    "        model=gru_model,\n",
    "        optimizer=optim.Adam(gru_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device='cpu'\n",
    "    ),\n",
    "  \n",
    "    Agent(\n",
    "        model=mamba_model,\n",
    "        optimizer=optim.Adam(mamba_model.parameters(), lr=0.1),\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        device='cuda'\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Main loop for training, fusion, feedback, and optimization\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "accuracy_scores = []\n",
    "agent_outputs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_predict_wrapper(agent, X):\n",
    "    # 确保输入是正确的\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    return agent.model(X).detach().cpu().numpy()  # 返回的是一个 numpy 数组\n",
    "num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 0, Loss: 3.0128660202026367\n",
      "Training Agent 2\n",
      "Batch 0, Loss: 2.5996286869049072\n",
      "Training Agent 3\n",
      "Batch 0, Loss: 16497.56640625\n",
      "Training Agent 4\n",
      "Batch 0, Loss: 3.0238425731658936\n",
      "Training Agent 5\n",
      "Batch 0, Loss: 2.7474365234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 1, Loss: 2.854903221130371\n",
      "Training Agent 2\n",
      "Batch 1, Loss: 2.563863515853882\n",
      "Training Agent 3\n",
      "Batch 1, Loss: 2.721766948699951\n",
      "Training Agent 4\n",
      "Batch 1, Loss: 2.7251136302948\n",
      "Training Agent 5\n",
      "Batch 1, Loss: 2.6834716796875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 2, Loss: 2.8059911727905273\n",
      "Training Agent 2\n",
      "Batch 2, Loss: 2.6441259384155273\n",
      "Training Agent 3\n",
      "Batch 2, Loss: 2.64510178565979\n",
      "Training Agent 4\n",
      "Batch 2, Loss: 2.627420425415039\n",
      "Training Agent 5\n",
      "Batch 2, Loss: 5.542157173156738\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 3, Loss: 2.5871505737304688\n",
      "Training Agent 2\n",
      "Batch 3, Loss: 2.4706335067749023\n",
      "Training Agent 3\n",
      "Batch 3, Loss: 2.6059274673461914\n",
      "Training Agent 4\n",
      "Batch 3, Loss: 2.470240354537964\n",
      "Training Agent 5\n",
      "Batch 3, Loss: 6.263885498046875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 4, Loss: 2.8757426738739014\n",
      "Training Agent 2\n",
      "Batch 4, Loss: 2.305708169937134\n",
      "Training Agent 3\n",
      "Batch 4, Loss: 2.620593309402466\n",
      "Training Agent 4\n",
      "Batch 4, Loss: 2.352277994155884\n",
      "Training Agent 5\n",
      "Batch 4, Loss: 3.39471435546875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 5, Loss: 2.8701090812683105\n",
      "Training Agent 2\n",
      "Batch 5, Loss: 2.2310521602630615\n",
      "Training Agent 3\n",
      "Batch 5, Loss: 2.527864933013916\n",
      "Training Agent 4\n",
      "Batch 5, Loss: 2.125143051147461\n",
      "Training Agent 5\n",
      "Batch 5, Loss: 7.0889892578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 6, Loss: 2.808345317840576\n",
      "Training Agent 2\n",
      "Batch 6, Loss: 2.2449748516082764\n",
      "Training Agent 3\n",
      "Batch 6, Loss: 2.435699224472046\n",
      "Training Agent 4\n",
      "Batch 6, Loss: 2.214580774307251\n",
      "Training Agent 5\n",
      "Batch 6, Loss: 6.719482421875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 7, Loss: 2.962350368499756\n",
      "Training Agent 2\n",
      "Batch 7, Loss: 2.1319777965545654\n",
      "Training Agent 3\n",
      "Batch 7, Loss: 2.4702370166778564\n",
      "Training Agent 4\n",
      "Batch 7, Loss: 2.1423518657684326\n",
      "Training Agent 5\n",
      "Batch 7, Loss: 6.207975387573242\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 8, Loss: 2.990776300430298\n",
      "Training Agent 2\n",
      "Batch 8, Loss: 1.9417544603347778\n",
      "Training Agent 3\n",
      "Batch 8, Loss: 2.378009796142578\n",
      "Training Agent 4\n",
      "Batch 8, Loss: 1.9416372776031494\n",
      "Training Agent 5\n",
      "Batch 8, Loss: 4.2135009765625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 9, Loss: 3.0975615978240967\n",
      "Training Agent 2\n",
      "Batch 9, Loss: 1.8951066732406616\n",
      "Training Agent 3\n",
      "Batch 9, Loss: 2.198082447052002\n",
      "Training Agent 4\n",
      "Batch 9, Loss: 1.8716099262237549\n",
      "Training Agent 5\n",
      "Batch 9, Loss: 6.732967376708984\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 10, Loss: 2.8245630264282227\n",
      "Training Agent 2\n",
      "Batch 10, Loss: 2.1014864444732666\n",
      "Training Agent 3\n",
      "Batch 10, Loss: 2.2590839862823486\n",
      "Training Agent 4\n",
      "Batch 10, Loss: 2.161829710006714\n",
      "Training Agent 5\n",
      "Batch 10, Loss: 7.237970352172852\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 11, Loss: 2.8488640785217285\n",
      "Training Agent 2\n",
      "Batch 11, Loss: 2.313237428665161\n",
      "Training Agent 3\n",
      "Batch 11, Loss: 2.3109018802642822\n",
      "Training Agent 4\n",
      "Batch 11, Loss: 2.428300380706787\n",
      "Training Agent 5\n",
      "Batch 11, Loss: 3.978363037109375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 12, Loss: 2.7583751678466797\n",
      "Training Agent 2\n",
      "Batch 12, Loss: 1.8412182331085205\n",
      "Training Agent 3\n",
      "Batch 12, Loss: 2.033830404281616\n",
      "Training Agent 4\n",
      "Batch 12, Loss: 1.8344208002090454\n",
      "Training Agent 5\n",
      "Batch 12, Loss: 5.116193771362305\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 13, Loss: 2.624239444732666\n",
      "Training Agent 2\n",
      "Batch 13, Loss: 2.461667060852051\n",
      "Training Agent 3\n",
      "Batch 13, Loss: 2.3324294090270996\n",
      "Training Agent 4\n",
      "Batch 13, Loss: 2.5509190559387207\n",
      "Training Agent 5\n",
      "Batch 13, Loss: 5.8134765625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 14, Loss: 2.9129135608673096\n",
      "Training Agent 2\n",
      "Batch 14, Loss: 1.6705455780029297\n",
      "Training Agent 3\n",
      "Batch 14, Loss: 1.9276397228240967\n",
      "Training Agent 4\n",
      "Batch 14, Loss: 1.6478511095046997\n",
      "Training Agent 5\n",
      "Batch 14, Loss: 5.0889892578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 15, Loss: 2.8879644870758057\n",
      "Training Agent 2\n",
      "Batch 15, Loss: 2.753812551498413\n",
      "Training Agent 3\n",
      "Batch 15, Loss: 2.482743263244629\n",
      "Training Agent 4\n",
      "Batch 15, Loss: 2.8953793048858643\n",
      "Training Agent 5\n",
      "Batch 15, Loss: 7.167579650878906\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 16, Loss: 3.023871660232544\n",
      "Training Agent 2\n",
      "Batch 16, Loss: 2.420605182647705\n",
      "Training Agent 3\n",
      "Batch 16, Loss: 2.3406789302825928\n",
      "Training Agent 4\n",
      "Batch 16, Loss: 2.4922494888305664\n",
      "Training Agent 5\n",
      "Batch 16, Loss: 6.1667633056640625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 17, Loss: 2.85296630859375\n",
      "Training Agent 2\n",
      "Batch 17, Loss: 2.3563895225524902\n",
      "Training Agent 3\n",
      "Batch 17, Loss: 2.3262860774993896\n",
      "Training Agent 4\n",
      "Batch 17, Loss: 2.351269483566284\n",
      "Training Agent 5\n",
      "Batch 17, Loss: 8.798248291015625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 18, Loss: 2.82603120803833\n",
      "Training Agent 2\n",
      "Batch 18, Loss: 2.064422607421875\n",
      "Training Agent 3\n",
      "Batch 18, Loss: 2.0393590927124023\n",
      "Training Agent 4\n",
      "Batch 18, Loss: 2.0214593410491943\n",
      "Training Agent 5\n",
      "Batch 18, Loss: 5.11798095703125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 19, Loss: 2.892143964767456\n",
      "Training Agent 2\n",
      "Batch 19, Loss: 2.12748122215271\n",
      "Training Agent 3\n",
      "Batch 19, Loss: 2.153642177581787\n",
      "Training Agent 4\n",
      "Batch 19, Loss: 1.996116042137146\n",
      "Training Agent 5\n",
      "Batch 19, Loss: 3.55035400390625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 20, Loss: 2.85095477104187\n",
      "Training Agent 2\n",
      "Batch 20, Loss: 1.7385385036468506\n",
      "Training Agent 3\n",
      "Batch 20, Loss: 1.8613940477371216\n",
      "Training Agent 4\n",
      "Batch 20, Loss: 1.6986610889434814\n",
      "Training Agent 5\n",
      "Batch 20, Loss: 5.24444580078125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 21, Loss: 2.776303768157959\n",
      "Training Agent 2\n",
      "Batch 21, Loss: 1.99075448513031\n",
      "Training Agent 3\n",
      "Batch 21, Loss: 1.9733433723449707\n",
      "Training Agent 4\n",
      "Batch 21, Loss: 1.9822839498519897\n",
      "Training Agent 5\n",
      "Batch 21, Loss: 4.34136962890625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 22, Loss: 2.993035316467285\n",
      "Training Agent 2\n",
      "Batch 22, Loss: 1.9295324087142944\n",
      "Training Agent 3\n",
      "Batch 22, Loss: 2.0116934776306152\n",
      "Training Agent 4\n",
      "Batch 22, Loss: 1.9057977199554443\n",
      "Training Agent 5\n",
      "Batch 22, Loss: 4.710205078125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 23, Loss: 2.965855598449707\n",
      "Training Agent 2\n",
      "Batch 23, Loss: 1.9424433708190918\n",
      "Training Agent 3\n",
      "Batch 23, Loss: 2.0072860717773438\n",
      "Training Agent 4\n",
      "Batch 23, Loss: 1.899389386177063\n",
      "Training Agent 5\n",
      "Batch 23, Loss: 4.456146240234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 24, Loss: 2.821417808532715\n",
      "Training Agent 2\n",
      "Batch 24, Loss: 1.7182763814926147\n",
      "Training Agent 3\n",
      "Batch 24, Loss: 1.7609591484069824\n",
      "Training Agent 4\n",
      "Batch 24, Loss: 1.781244158744812\n",
      "Training Agent 5\n",
      "Batch 24, Loss: 2.66583251953125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 25, Loss: 2.935433864593506\n",
      "Training Agent 2\n",
      "Batch 25, Loss: 1.73082435131073\n",
      "Training Agent 3\n",
      "Batch 25, Loss: 1.7514857053756714\n",
      "Training Agent 4\n",
      "Batch 25, Loss: 1.8024300336837769\n",
      "Training Agent 5\n",
      "Batch 25, Loss: 3.3563156127929688\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 26, Loss: 2.71281099319458\n",
      "Training Agent 2\n",
      "Batch 26, Loss: 2.1488051414489746\n",
      "Training Agent 3\n",
      "Batch 26, Loss: 2.2138876914978027\n",
      "Training Agent 4\n",
      "Batch 26, Loss: 2.1547935009002686\n",
      "Training Agent 5\n",
      "Batch 26, Loss: 7.0186767578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 27, Loss: 2.7704029083251953\n",
      "Training Agent 2\n",
      "Batch 27, Loss: 1.9733664989471436\n",
      "Training Agent 3\n",
      "Batch 27, Loss: 1.9748762845993042\n",
      "Training Agent 4\n",
      "Batch 27, Loss: 2.0491251945495605\n",
      "Training Agent 5\n",
      "Batch 27, Loss: 2.53472900390625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 28, Loss: 2.8104944229125977\n",
      "Training Agent 2\n",
      "Batch 28, Loss: 1.80405855178833\n",
      "Training Agent 3\n",
      "Batch 28, Loss: 1.7425965070724487\n",
      "Training Agent 4\n",
      "Batch 28, Loss: 1.7202239036560059\n",
      "Training Agent 5\n",
      "Batch 28, Loss: 3.904754638671875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 29, Loss: 2.8513317108154297\n",
      "Training Agent 2\n",
      "Batch 29, Loss: 1.6639865636825562\n",
      "Training Agent 3\n",
      "Batch 29, Loss: 1.6500656604766846\n",
      "Training Agent 4\n",
      "Batch 29, Loss: 1.610992193222046\n",
      "Training Agent 5\n",
      "Batch 29, Loss: 4.135498046875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 30, Loss: 2.7476911544799805\n",
      "Training Agent 2\n",
      "Batch 30, Loss: 1.8586825132369995\n",
      "Training Agent 3\n",
      "Batch 30, Loss: 1.9836294651031494\n",
      "Training Agent 4\n",
      "Batch 30, Loss: 1.9930753707885742\n",
      "Training Agent 5\n",
      "Batch 30, Loss: 2.58306884765625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 31, Loss: 2.8262288570404053\n",
      "Training Agent 2\n",
      "Batch 31, Loss: 1.6608160734176636\n",
      "Training Agent 3\n",
      "Batch 31, Loss: 1.6266205310821533\n",
      "Training Agent 4\n",
      "Batch 31, Loss: 1.5890589952468872\n",
      "Training Agent 5\n",
      "Batch 31, Loss: 3.1829757690429688\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 32, Loss: 2.912142753601074\n",
      "Training Agent 2\n",
      "Batch 32, Loss: 1.5941698551177979\n",
      "Training Agent 3\n",
      "Batch 32, Loss: 1.5991350412368774\n",
      "Training Agent 4\n",
      "Batch 32, Loss: 1.6320000886917114\n",
      "Training Agent 5\n",
      "Batch 32, Loss: 2.0595855712890625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 33, Loss: 2.881002426147461\n",
      "Training Agent 2\n",
      "Batch 33, Loss: 1.7246530055999756\n",
      "Training Agent 3\n",
      "Batch 33, Loss: 1.7784262895584106\n",
      "Training Agent 4\n",
      "Batch 33, Loss: 1.741786241531372\n",
      "Training Agent 5\n",
      "Batch 33, Loss: 2.512908935546875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 34, Loss: 2.913137197494507\n",
      "Training Agent 2\n",
      "Batch 34, Loss: 1.916860580444336\n",
      "Training Agent 3\n",
      "Batch 34, Loss: 1.9288934469223022\n",
      "Training Agent 4\n",
      "Batch 34, Loss: 1.9920967817306519\n",
      "Training Agent 5\n",
      "Batch 34, Loss: 3.5714492797851562\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 35, Loss: 3.0483720302581787\n",
      "Training Agent 2\n",
      "Batch 35, Loss: 1.8589924573898315\n",
      "Training Agent 3\n",
      "Batch 35, Loss: 1.838623285293579\n",
      "Training Agent 4\n",
      "Batch 35, Loss: 1.8759703636169434\n",
      "Training Agent 5\n",
      "Batch 35, Loss: 2.1868896484375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 36, Loss: nan\n",
      "Training Agent 2\n",
      "Batch 36, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 36, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 36, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 36, Loss: 3.930023193359375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 37, Loss: 3.05537486076355\n",
      "Training Agent 2\n",
      "Batch 37, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 37, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 37, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 37, Loss: 2.9613037109375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 38, Loss: 2.719093084335327\n",
      "Training Agent 2\n",
      "Batch 38, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 38, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 38, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 38, Loss: 3.853130340576172\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 39, Loss: 2.602245330810547\n",
      "Training Agent 2\n",
      "Batch 39, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 39, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 39, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 39, Loss: 2.8771286010742188\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 40, Loss: 2.8019649982452393\n",
      "Training Agent 2\n",
      "Batch 40, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 40, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 40, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 40, Loss: 3.8086318969726562\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 41, Loss: 2.899218797683716\n",
      "Training Agent 2\n",
      "Batch 41, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 41, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 41, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 41, Loss: 2.967193603515625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 42, Loss: 3.0734996795654297\n",
      "Training Agent 2\n",
      "Batch 42, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 42, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 42, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 42, Loss: 2.59844970703125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 43, Loss: 2.9654834270477295\n",
      "Training Agent 2\n",
      "Batch 43, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 43, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 43, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 43, Loss: 2.799591064453125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 44, Loss: 3.142728090286255\n",
      "Training Agent 2\n",
      "Batch 44, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 44, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 44, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 44, Loss: 3.3165283203125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 45, Loss: 2.8688580989837646\n",
      "Training Agent 2\n",
      "Batch 45, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 45, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 45, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 45, Loss: 3.643585205078125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 46, Loss: 2.9545202255249023\n",
      "Training Agent 2\n",
      "Batch 46, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 46, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 46, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 46, Loss: 3.09771728515625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 47, Loss: 2.704469680786133\n",
      "Training Agent 2\n",
      "Batch 47, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 47, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 47, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 47, Loss: 3.2161865234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 48, Loss: 2.573352575302124\n",
      "Training Agent 2\n",
      "Batch 48, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 48, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 48, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 48, Loss: 2.6765823364257812\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 49, Loss: 2.7846364974975586\n",
      "Training Agent 2\n",
      "Batch 49, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 49, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 49, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 49, Loss: 2.6529541015625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 50, Loss: 2.855569362640381\n",
      "Training Agent 2\n",
      "Batch 50, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 50, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 50, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 50, Loss: 3.4171905517578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 51, Loss: 3.0559449195861816\n",
      "Training Agent 2\n",
      "Batch 51, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 51, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 51, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 51, Loss: 6.10870361328125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 52, Loss: 2.7592926025390625\n",
      "Training Agent 2\n",
      "Batch 52, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 52, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 52, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 52, Loss: 2.105712890625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 53, Loss: 2.8549599647521973\n",
      "Training Agent 2\n",
      "Batch 53, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 53, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 53, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 53, Loss: 2.3602294921875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 54, Loss: 2.9617793560028076\n",
      "Training Agent 2\n",
      "Batch 54, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 54, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 54, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 54, Loss: 1.815338134765625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 55, Loss: 2.7812931537628174\n",
      "Training Agent 2\n",
      "Batch 55, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 55, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 55, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 55, Loss: 2.466217041015625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 56, Loss: 2.9629740715026855\n",
      "Training Agent 2\n",
      "Batch 56, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 56, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 56, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 56, Loss: 3.9860763549804688\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 57, Loss: 2.7735462188720703\n",
      "Training Agent 2\n",
      "Batch 57, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 57, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 57, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 57, Loss: 8.820565223693848\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 58, Loss: 2.9429943561553955\n",
      "Training Agent 2\n",
      "Batch 58, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 58, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 58, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 58, Loss: 2.862884521484375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 59, Loss: 2.983603000640869\n",
      "Training Agent 2\n",
      "Batch 59, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 59, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 59, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 59, Loss: 4.40753173828125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 60, Loss: 3.0380523204803467\n",
      "Training Agent 2\n",
      "Batch 60, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 60, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 60, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 60, Loss: 2.060577392578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 61, Loss: 3.0319995880126953\n",
      "Training Agent 2\n",
      "Batch 61, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 61, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 61, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 61, Loss: 3.64813232421875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 62, Loss: 2.9821176528930664\n",
      "Training Agent 2\n",
      "Batch 62, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 62, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 62, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 62, Loss: 2.630859375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 63, Loss: 2.8290481567382812\n",
      "Training Agent 2\n",
      "Batch 63, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 63, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 63, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 63, Loss: 2.81671142578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 64, Loss: 2.703096389770508\n",
      "Training Agent 2\n",
      "Batch 64, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 64, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 64, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 64, Loss: 3.7401123046875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 65, Loss: 2.9156787395477295\n",
      "Training Agent 2\n",
      "Batch 65, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 65, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 65, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 65, Loss: 2.17108154296875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 66, Loss: 2.8914499282836914\n",
      "Training Agent 2\n",
      "Batch 66, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 66, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 66, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 66, Loss: 2.8848648071289062\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 67, Loss: 2.784345865249634\n",
      "Training Agent 2\n",
      "Batch 67, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 67, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 67, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 67, Loss: 3.20123291015625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 68, Loss: 2.7853035926818848\n",
      "Training Agent 2\n",
      "Batch 68, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 68, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 68, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 68, Loss: 1.81298828125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 69, Loss: 2.957221508026123\n",
      "Training Agent 2\n",
      "Batch 69, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 69, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 69, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 69, Loss: 5.001678466796875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 70, Loss: 2.965899705886841\n",
      "Training Agent 2\n",
      "Batch 70, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 70, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 70, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 70, Loss: 2.988555908203125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 71, Loss: 2.90664005279541\n",
      "Training Agent 2\n",
      "Batch 71, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 71, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 71, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 71, Loss: 2.678802490234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 72, Loss: 3.0039355754852295\n",
      "Training Agent 2\n",
      "Batch 72, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 72, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 72, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 72, Loss: 2.77886962890625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 73, Loss: 3.003948926925659\n",
      "Training Agent 2\n",
      "Batch 73, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 73, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 73, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 73, Loss: 2.478668212890625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 74, Loss: 2.708967685699463\n",
      "Training Agent 2\n",
      "Batch 74, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 74, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 74, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 74, Loss: 2.800872802734375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 75, Loss: 2.736050605773926\n",
      "Training Agent 2\n",
      "Batch 75, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 75, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 75, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 75, Loss: 2.429718017578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 76, Loss: 2.888699531555176\n",
      "Training Agent 2\n",
      "Batch 76, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 76, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 76, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 76, Loss: 1.749603271484375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 77, Loss: 2.791412353515625\n",
      "Training Agent 2\n",
      "Batch 77, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 77, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 77, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 77, Loss: 2.261962890625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 78, Loss: 2.892435312271118\n",
      "Training Agent 2\n",
      "Batch 78, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 78, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 78, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 78, Loss: 4.29779052734375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 79, Loss: 2.7636120319366455\n",
      "Training Agent 2\n",
      "Batch 79, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 79, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 79, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 79, Loss: 3.93115234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 80, Loss: 2.7393345832824707\n",
      "Training Agent 2\n",
      "Batch 80, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 80, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 80, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 80, Loss: 2.4515380859375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 81, Loss: 2.8709404468536377\n",
      "Training Agent 2\n",
      "Batch 81, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 81, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 81, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 81, Loss: 2.6595458984375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 82, Loss: 2.886747121810913\n",
      "Training Agent 2\n",
      "Batch 82, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 82, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 82, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 82, Loss: 2.63958740234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 83, Loss: 2.974435806274414\n",
      "Training Agent 2\n",
      "Batch 83, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 83, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 83, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 83, Loss: 1.6661376953125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 84, Loss: 2.769289970397949\n",
      "Training Agent 2\n",
      "Batch 84, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 84, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 84, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 84, Loss: 2.790069580078125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 85, Loss: 2.6808371543884277\n",
      "Training Agent 2\n",
      "Batch 85, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 85, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 85, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 85, Loss: 1.762939453125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 86, Loss: 2.831184148788452\n",
      "Training Agent 2\n",
      "Batch 86, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 86, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 86, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 86, Loss: 3.43365478515625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 87, Loss: 2.9739599227905273\n",
      "Training Agent 2\n",
      "Batch 87, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 87, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 87, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 87, Loss: 1.79150390625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 88, Loss: 2.8164281845092773\n",
      "Training Agent 2\n",
      "Batch 88, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 88, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 88, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 88, Loss: 2.78826904296875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 89, Loss: 2.9038729667663574\n",
      "Training Agent 2\n",
      "Batch 89, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 89, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 89, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 89, Loss: 2.04876708984375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 90, Loss: 2.97011137008667\n",
      "Training Agent 2\n",
      "Batch 90, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 90, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 90, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 90, Loss: 2.74639892578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 91, Loss: 2.764521598815918\n",
      "Training Agent 2\n",
      "Batch 91, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 91, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 91, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 91, Loss: 2.41632080078125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 92, Loss: 3.003457546234131\n",
      "Training Agent 2\n",
      "Batch 92, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 92, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 92, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 92, Loss: 3.6527175903320312\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 93, Loss: 2.703341484069824\n",
      "Training Agent 2\n",
      "Batch 93, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 93, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 93, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 93, Loss: 2.4071044921875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 94, Loss: 2.887685537338257\n",
      "Training Agent 2\n",
      "Batch 94, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 94, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 94, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 94, Loss: 2.08477783203125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 95, Loss: 2.734062433242798\n",
      "Training Agent 2\n",
      "Batch 95, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 95, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 95, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 95, Loss: 2.246124267578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 96, Loss: 2.954399824142456\n",
      "Training Agent 2\n",
      "Batch 96, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 96, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 96, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 96, Loss: 2.60955810546875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 97, Loss: 2.818567991256714\n",
      "Training Agent 2\n",
      "Batch 97, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 97, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 97, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 97, Loss: 2.8270263671875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 98, Loss: 2.767688512802124\n",
      "Training Agent 2\n",
      "Batch 98, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 98, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 98, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 98, Loss: 2.2144775390625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 99, Loss: 2.7781081199645996\n",
      "Training Agent 2\n",
      "Batch 99, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 99, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 99, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 99, Loss: 2.460174560546875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 100, Loss: 2.963608503341675\n",
      "Training Agent 2\n",
      "Batch 100, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 100, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 100, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 100, Loss: 3.0201416015625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 101, Loss: 2.811152696609497\n",
      "Training Agent 2\n",
      "Batch 101, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 101, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 101, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 101, Loss: 1.949676513671875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 102, Loss: 2.944539785385132\n",
      "Training Agent 2\n",
      "Batch 102, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 102, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 102, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 102, Loss: 3.055755615234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 103, Loss: 3.021078586578369\n",
      "Training Agent 2\n",
      "Batch 103, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 103, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 103, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 103, Loss: 3.9739837646484375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 104, Loss: 3.231231451034546\n",
      "Training Agent 2\n",
      "Batch 104, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 104, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 104, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 104, Loss: 2.6400146484375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 105, Loss: 2.81300950050354\n",
      "Training Agent 2\n",
      "Batch 105, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 105, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 105, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 105, Loss: 4.149604797363281\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 106, Loss: 2.947287082672119\n",
      "Training Agent 2\n",
      "Batch 106, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 106, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 106, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 106, Loss: 2.8701171875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 107, Loss: 3.0752506256103516\n",
      "Training Agent 2\n",
      "Batch 107, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 107, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 107, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 107, Loss: 7.88189697265625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 108, Loss: 2.9049668312072754\n",
      "Training Agent 2\n",
      "Batch 108, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 108, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 108, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 108, Loss: 3.0123291015625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 109, Loss: 2.755016565322876\n",
      "Training Agent 2\n",
      "Batch 109, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 109, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 109, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 109, Loss: 2.8513412475585938\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 110, Loss: 2.9366087913513184\n",
      "Training Agent 2\n",
      "Batch 110, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 110, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 110, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 110, Loss: 3.6981658935546875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 111, Loss: 2.6699390411376953\n",
      "Training Agent 2\n",
      "Batch 111, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 111, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 111, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 111, Loss: 2.290313720703125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 112, Loss: 2.9783589839935303\n",
      "Training Agent 2\n",
      "Batch 112, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 112, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 112, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 112, Loss: 2.380859375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 113, Loss: 2.8595497608184814\n",
      "Training Agent 2\n",
      "Batch 113, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 113, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 113, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 113, Loss: 3.2403564453125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 114, Loss: 2.909788131713867\n",
      "Training Agent 2\n",
      "Batch 114, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 114, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 114, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 114, Loss: 3.45361328125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 115, Loss: 2.913911819458008\n",
      "Training Agent 2\n",
      "Batch 115, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 115, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 115, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 115, Loss: 3.129608154296875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 116, Loss: 2.9235854148864746\n",
      "Training Agent 2\n",
      "Batch 116, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 116, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 116, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 116, Loss: 2.744384765625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 117, Loss: 2.9792399406433105\n",
      "Training Agent 2\n",
      "Batch 117, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 117, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 117, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 117, Loss: 2.9986572265625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 118, Loss: 2.9371752738952637\n",
      "Training Agent 2\n",
      "Batch 118, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 118, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 118, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 118, Loss: 2.1171875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 119, Loss: 2.903982639312744\n",
      "Training Agent 2\n",
      "Batch 119, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 119, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 119, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 119, Loss: 2.2000274658203125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 120, Loss: 2.979250907897949\n",
      "Training Agent 2\n",
      "Batch 120, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 120, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 120, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 120, Loss: 2.8895263671875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 121, Loss: 2.992204427719116\n",
      "Training Agent 2\n",
      "Batch 121, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 121, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 121, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 121, Loss: 1.36474609375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 122, Loss: 2.963214874267578\n",
      "Training Agent 2\n",
      "Batch 122, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 122, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 122, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 122, Loss: 3.5882568359375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 123, Loss: 2.702760934829712\n",
      "Training Agent 2\n",
      "Batch 123, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 123, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 123, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 123, Loss: 2.68072509765625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 124, Loss: 2.9430363178253174\n",
      "Training Agent 2\n",
      "Batch 124, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 124, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 124, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 124, Loss: 2.71038818359375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 125, Loss: 2.7325305938720703\n",
      "Training Agent 2\n",
      "Batch 125, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 125, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 125, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 125, Loss: 2.4853515625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 126, Loss: 2.7254295349121094\n",
      "Training Agent 2\n",
      "Batch 126, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 126, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 126, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 126, Loss: 5.2972412109375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 127, Loss: 3.026651382446289\n",
      "Training Agent 2\n",
      "Batch 127, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 127, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 127, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 127, Loss: 3.23162841796875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 128, Loss: 2.9955332279205322\n",
      "Training Agent 2\n",
      "Batch 128, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 128, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 128, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 128, Loss: 2.824066162109375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 129, Loss: 2.882524013519287\n",
      "Training Agent 2\n",
      "Batch 129, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 129, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 129, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 129, Loss: 4.4085693359375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 130, Loss: 2.8379814624786377\n",
      "Training Agent 2\n",
      "Batch 130, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 130, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 130, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 130, Loss: 3.92388916015625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 131, Loss: 2.693784475326538\n",
      "Training Agent 2\n",
      "Batch 131, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 131, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 131, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 131, Loss: 4.098381042480469\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 132, Loss: 2.9068264961242676\n",
      "Training Agent 2\n",
      "Batch 132, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 132, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 132, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 132, Loss: 3.41448974609375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 133, Loss: 2.7333590984344482\n",
      "Training Agent 2\n",
      "Batch 133, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 133, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 133, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 133, Loss: 3.299560546875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 134, Loss: 2.7414960861206055\n",
      "Training Agent 2\n",
      "Batch 134, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 134, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 134, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 134, Loss: 3.4450836181640625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 135, Loss: 2.7855935096740723\n",
      "Training Agent 2\n",
      "Batch 135, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 135, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 135, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 135, Loss: 3.540679931640625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 136, Loss: 2.7879693508148193\n",
      "Training Agent 2\n",
      "Batch 136, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 136, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 136, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 136, Loss: 4.541473388671875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 137, Loss: 2.8147103786468506\n",
      "Training Agent 2\n",
      "Batch 137, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 137, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 137, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 137, Loss: 3.3245849609375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 138, Loss: 3.0843300819396973\n",
      "Training Agent 2\n",
      "Batch 138, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 138, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 138, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 138, Loss: 2.5559005737304688\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 139, Loss: 2.786268711090088\n",
      "Training Agent 2\n",
      "Batch 139, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 139, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 139, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 139, Loss: 3.392292022705078\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 140, Loss: 2.7479541301727295\n",
      "Training Agent 2\n",
      "Batch 140, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 140, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 140, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 140, Loss: 3.70245361328125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 141, Loss: 2.854820489883423\n",
      "Training Agent 2\n",
      "Batch 141, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 141, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 141, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 141, Loss: 3.8408203125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 142, Loss: 2.982994794845581\n",
      "Training Agent 2\n",
      "Batch 142, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 142, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 142, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 142, Loss: 2.2379150390625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 143, Loss: 2.812295913696289\n",
      "Training Agent 2\n",
      "Batch 143, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 143, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 143, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 143, Loss: 2.7747802734375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 144, Loss: 2.7873740196228027\n",
      "Training Agent 2\n",
      "Batch 144, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 144, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 144, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 144, Loss: 3.722759246826172\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 145, Loss: 3.1572108268737793\n",
      "Training Agent 2\n",
      "Batch 145, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 145, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 145, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 145, Loss: 4.2609405517578125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 146, Loss: 2.922922134399414\n",
      "Training Agent 2\n",
      "Batch 146, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 146, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 146, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 146, Loss: 2.131622314453125\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 147, Loss: 2.8771209716796875\n",
      "Training Agent 2\n",
      "Batch 147, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 147, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 147, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 147, Loss: 2.9722900390625\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 148, Loss: 2.8621673583984375\n",
      "Training Agent 2\n",
      "Batch 148, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 148, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 148, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 148, Loss: 2.5379638671875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 149, Loss: 2.831174850463867\n",
      "Training Agent 2\n",
      "Batch 149, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 149, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 149, Loss: nan\n",
      "Training Agent 5\n",
      "Batch 149, Loss: 3.43115234375\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x794d642ddfd0>\n",
      "Training Agent 1\n",
      "Batch 150, Loss: 3.0650129318237305\n",
      "Training Agent 2\n",
      "Batch 150, Loss: nan\n",
      "Training Agent 3\n",
      "Batch 150, Loss: nan\n",
      "Training Agent 4\n",
      "Batch 150, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m             losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(agent_loss))  \u001b[38;5;66;03m# 记录当前agent每个epoch的loss平均值\u001b[39;00m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;66;03m# Generate predictions for FB distribution\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m             agent_outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;66;03m# 每个epoch结束后保存模型\u001b[39;00m\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;66;03m#save_model(agent, i, 1)     \u001b[39;00m\n\u001b[1;32m     39\u001b[0m fb_distribution \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(agent_outputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mAgent.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 模型前向传播\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/data/model/gru.py:11\u001b[0m, in \u001b[0;36mGRUModel.forward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_seq):\n\u001b[0;32m---> 11\u001b[0m     gru_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(gru_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Only use last time step's output\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:955\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    959\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    # 记录loss\n",
    "    agent_outputs = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(train_loader)\n",
    "        losses = []\n",
    "        for i, agent in enumerate(agents):\n",
    "            if i==0:\n",
    "                print(f\"Training Agent {i + 1}\")\n",
    "                agent_loss = []\n",
    "                #print(data)\n",
    "                #print(target)\n",
    "                with torch.cuda.amp.autocast():  # 使用混合精度\n",
    "                    loss = agent.train_NeuralODE(data, target)\n",
    "                agent_loss.append(loss)  # 将每个batch的loss存入列表\n",
    "                print(f\"Batch {batch_idx}, Loss: {loss}\")\n",
    "                \n",
    "                losses.append(np.mean([l.cpu().detach().numpy() for l in agent_loss]))  # 记录当前agent每个epoch的loss平均值\n",
    "                agent_outputs.append(agent.predict_NeuralODE(X_test))\n",
    "                # 每个epoch结束后保存模型\n",
    "                #save_model(agent, i, 1)    \n",
    "            else:\n",
    "                print(f\"Training Agent {i + 1}\")\n",
    "                agent_loss = []\n",
    "                #print(f\"Batch {batch_idx}, Data: {data}\")  # 打印数据结构\n",
    "            \n",
    "                with torch.cuda.amp.autocast():  # 使用混合精度\n",
    "                    loss = agent.train(data, target)\n",
    "                agent_loss.append(loss)  # 将每个batch的loss存入列表\n",
    "                print(f\"Batch {batch_idx}, Loss: {loss}\")\n",
    "                # 将当前agent的所有loss加入到全局loss记录中\n",
    "                losses.append(np.mean(agent_loss))  # 记录当前agent每个epoch的loss平均值\n",
    "                # Generate predictions for FB distribution\n",
    "                agent_outputs.append(agent.predict(X_test))\n",
    "                # 每个epoch结束后保存模型\n",
    "                #save_model(agent, i, 1)     \n",
    " \n",
    "    fb_distribution = torch.mean(torch.stack(agent_outputs), dim=0)\n",
    "\n",
    "    for i, agent in enumerate(agents):\n",
    "        if i >= len(agent_outputs):  # 防止越界访问\n",
    "            print(f\"Warning: agent_outputs does not have an output for agent {i + 1}\")\n",
    "            continue  # 跳过当前 agent\n",
    "        # 确保 agent_outputs 和 fb_distribution 是 Torch 张量并且需要梯度\n",
    "        raw_output = torch.tensor(agent_outputs[i], dtype=torch.float32, requires_grad=True)  # 使 raw_output 具有梯度\n",
    "        fb_distribution_tensor = fb_distribution  # 此时 fb_distribution 已经是 Torch 张量，继续使用\n",
    "        # 使用损失函数计算损失\n",
    "        fb_loss = agent.loss_fn(raw_output, fb_distribution_tensor)\n",
    "        # 反向传播\n",
    "        agent.optimizer.zero_grad()\n",
    "        fb_loss.backward()  # 现在可以正常计算梯度\n",
    "        agent.optimizer.step()\n",
    "    # Shapley value calculation\n",
    "    shapley_values = []\n",
    "    for i, agent in enumerate(agents):\n",
    "        # Randomly sample from X_train for Shapley calculation\n",
    "        if(i==0):\n",
    "            sample_idx = np.random.choice(len(X_train), size=100, replace=False)\n",
    "            X_sample = X_train[sample_idx]\n",
    "            explainer = shap.KernelExplainer(agent.predict_NeuralODE, X_sample)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shapley_values.append(np.mean(shap_values))\n",
    "        else:\n",
    "            sample_idx = np.random.choice(len(X_train), size=100, replace=False)\n",
    "            X_sample = X_train[sample_idx]\n",
    "            explainer = shap.KernelExplainer(agent.predict, X_sample)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shapley_values.append(np.mean(shap_values))\n",
    "    # Reward and learning rate adjustment\n",
    "    total_shapley = sum(shapley_values)\n",
    "    for i, agent in enumerate(agents):\n",
    "        reward = shapley_values[i] / total_shapley if total_shapley > 0 else 0\n",
    "        agent.update_reward(reward)\n",
    "        agent.adjust_weight()\n",
    "        agent.adjust_learning_rate()\n",
    "\n",
    "    # Display rewards\n",
    "    for i, agent in enumerate(agents):\n",
    "        print(f\"Agent {i + 1} reward after iteration {epoch + 1}: {agent.reward}\")\n",
    "    # epoch结束\n",
    "    print(f\"Epoch {epoch+1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#将模型进行封装\n",
    "class MultiAgentModel:\n",
    "    def __init__(self, agents, device='cuda'):\n",
    "        \"\"\"\n",
    "        封装多个代理模型\n",
    "        :param agents: 一个包含 Agent 实例的列表\n",
    "        :param device: 设备类型\n",
    "        \"\"\"\n",
    "        self.agents = agents\n",
    "        self.device = device\n",
    "        self.agent_weights = [1.0 / len(agents)] * len(agents)  # 初始均分权重\n",
    "\n",
    "    def train(self, train_loader, X_test, num_epochs=10):\n",
    "        \"\"\"\n",
    "        训练模型并基于 Shapley 值调整代理权重\n",
    "        :param train_loader: 训练数据加载器\n",
    "        :param X_test: 用于 Shapley 计算的测试数据\n",
    "        :param num_epochs: 训练轮数\n",
    "        \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # 每个代理独立训练\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                losses = []\n",
    "                for i, agent in enumerate(self.agents):\n",
    "                    if isinstance(agent.model, NeuralODE):\n",
    "                        loss = agent.train_NeuralODE(data, target)\n",
    "                    else:\n",
    "                        loss = agent.train(data, target)\n",
    "                    losses.append(loss)\n",
    "\n",
    "            # 计算 Shapley 值\n",
    "            shapley_values = self._calculate_shapley(X_test)\n",
    "            \n",
    "            # 根据 Shapley 值调整权重\n",
    "            total_shapley = sum(shapley_values)\n",
    "            if total_shapley > 0:\n",
    "                self.agent_weights = [val / total_shapley for val in shapley_values]\n",
    "            else:\n",
    "                self.agent_weights = [1.0 / len(self.agents)] * len(self.agents)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测结果，整合多个代理的预测值\n",
    "        :param X: 输入数据\n",
    "        :return: 整体预测结果\n",
    "        \"\"\"\n",
    "        final_output = None\n",
    "        for agent, weight in zip(self.agents, self.agent_weights):\n",
    "            if isinstance(agent.model, NeuralODE):\n",
    "                output = agent.predict_NeuralODE(X)\n",
    "            else:\n",
    "                output = agent.predict(X)\n",
    "            \n",
    "            weighted_output = weight * output  # 加权整合\n",
    "            if final_output is None:\n",
    "                final_output = weighted_output\n",
    "            else:\n",
    "                final_output += weighted_output\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "    def _calculate_shapley(self, X_test):\n",
    "        \"\"\"\n",
    "        计算 Shapley 值，用于衡量各代理的贡献\n",
    "        :param X_test: 测试数据\n",
    "        :return: 各代理的 Shapley 值\n",
    "        \"\"\"\n",
    "        shapley_values = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if isinstance(agent.model, NeuralODE):\n",
    "                explainer = shap.KernelExplainer(agent.predict_NeuralODE, X_test)\n",
    "            else:\n",
    "                explainer = shap.KernelExplainer(agent.predict, X_test)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shapley_values.append(np.mean(shap_values))\n",
    "        \n",
    "        return shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for agent_name, losses in loss_curves.items():\n",
    "    plt.plot(range(1, num_iterations + 1), losses, label=agent_name)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve for Each Agent')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvcc --version  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
